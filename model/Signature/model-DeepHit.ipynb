{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc667f12-52bb-4f15-848d-c6e8324187c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from scipy.integrate import trapz\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import LogisticHazard, PMF, DeepHitSingle, CoxPH, MTLR\n",
    "from pycox.evaluation import EvalSurv\n",
    "from survival_evaluation import d_calibration, l1, one_calibration\n",
    "import random\n",
    "import statistics\n",
    "from pdb import set_trace\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sksurv.metrics import cumulative_dynamic_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1b6120-9d7e-4b4e-b1ed-6e4dadebf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_scores = []\n",
    "\n",
    "# Read and process data\n",
    "path = '../../data/NASA-Turbofan/Signature/train_new.csv'\n",
    "path1 = '../../data/NASA-Turbofan/Signature/test_new.csv'\n",
    "\n",
    "def load_data(path):\n",
    "    D = pd.read_csv(path)\n",
    "    # Configure column names\n",
    "    x_cols = D.iloc[:, 4:].columns.tolist()  # Feature columns start from the 5th column\n",
    "    event_col = ['event']   # Survival event column\n",
    "    time_col = ['time']     # Survival time column\n",
    "    # Data cleaning and column selection\n",
    "    D = D[x_cols + event_col + time_col]\n",
    "    return D, x_cols\n",
    "\n",
    "# Load training and testing data\n",
    "d, x_cols = load_data(path)\n",
    "d1, x_cols = load_data(path1)\n",
    "\n",
    "# Columns to standardize\n",
    "cols_standardize = x_cols.copy()\n",
    "\n",
    "# Number of experiments\n",
    "n_exp = 30\n",
    "\n",
    "# Model evaluation metrics\n",
    "CI = []\n",
    "IBS = []\n",
    "L1_hinge = []\n",
    "L1_margin = []\n",
    "RMSE = []\n",
    "AUC = []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ac728e0",
   "metadata": {},
   "source": [
    "DeepHit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c10e7e-b588-4549-861e-4e4ac9ff7bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI: 0.845 0.035\n",
      "IBS: 0.055 0.017\n",
      "L1_hinge: 6.735 1.413\n",
      "L1_margin: 17.177 3.055\n",
      "AUC: 0.903 ± 0.042\n",
      "d_calibration_p_value: 0.0\n",
      "D-Calibration: 1.0\n",
      "d_calibration_bin_proportions:\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.04780835041633017\n",
      "0.43266929099192986\n",
      "D-Calibration_censored: 0.573\n",
      "d_calibration_censored_contributions:\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.06494029852060171\n",
      "0.04780835041633017\n",
      "0.005746214068852932\n",
      "D-Calibration_uncensored: 0.427\n",
      "d_calibration_uncensored_contributions:\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4269230769230769\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for i in range(n_exp):\n",
    "    #df_train, df_val, df_test = train_val_test_stratified_split(d, 'event', frac_train=0.80, frac_val=0.05, frac_test=0.15, random_state=10)\n",
    "    df_train=d\n",
    "    df_val=d\n",
    "    df_test=d1\n",
    "    standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "    x_mapper = DataFrameMapper(standardize)\n",
    "    x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "    x_val = x_mapper.transform(df_val).astype('float32')\n",
    "    x_test = x_mapper.transform(df_test).astype('float32')\n",
    "    in_features = x_train.shape[1]\n",
    "\n",
    "    num_durations = 10\n",
    "    labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "    get_target = lambda df: (df['time'].values, df['event'].values)\n",
    "    y_train = labtrans.fit_transform(*get_target(df_train))\n",
    "    y_val = labtrans.transform(*get_target(df_val))\n",
    "    val = tt.tuplefy(x_val, y_val)\n",
    "    durations_test, events_test = get_target(df_test)\n",
    "\n",
    "    out_features = labtrans.out_features\n",
    "    num_nodes = [32, 32]\n",
    "    batch_norm = True\n",
    "    dropout = 0.1\n",
    "    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n",
    "    model = DeepHitSingle(net, tt.optim.Adam, duration_index=labtrans.cuts)\n",
    "\n",
    "    batch_size = 256\n",
    "    lr_finder = model.lr_finder(x_train, y_train, batch_size, tolerance=6)\n",
    "    model.optimizer.set_lr(0.01)\n",
    "\n",
    "    epochs = 512\n",
    "    callbacks = [tt.cb.EarlyStopping()]\n",
    "    model.fit(x_train, y_train, batch_size, epochs, callbacks, val_data=val, verbose=0)\n",
    "\n",
    "    surv = model.interpolate(10).predict_surv_df(x_test)\n",
    "    surv_df = pd.DataFrame(surv)\n",
    "    surv_df.index.name = 'time'\n",
    "    surv_df.columns.name = 'survival_function'\n",
    " \n",
    "\n",
    "    survival_predictions = pd.Series(trapz(surv.values.T, surv.index), index=df_test.index)\n",
    "    l1_hinge_value = l1(df_test.time, df_test.event, survival_predictions, l1_type='hinge')\n",
    "    l1_margin_value = l1(df_test.time, df_test.event, survival_predictions, df_train.time, df_train.event, l1_type='margin')\n",
    "\n",
    "    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "    time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "    brier = ev.integrated_brier_score(time_grid)\n",
    "\n",
    "    quantiles = np.sort(df_test['time'].unique())\n",
    "    labels_train = np.array([(e, t) for e, t in zip(df_train['event'], df_train['time'])], dtype=[('event', 'bool'), ('time', 'float')])\n",
    "    labels_test = np.array([(e, t) for e, t in zip(df_test['event'], df_test['time'])], dtype=[('event', 'bool'), ('time', 'float')])\n",
    "\n",
    "    auc_scores = []\n",
    "    for eval_time in quantiles:\n",
    "        try:\n",
    "            # use surv.index instead of time_grid_train\n",
    "            interp_time_index = np.argmin(np.abs(eval_time - surv.index.values))\n",
    "            surv_values_at_eval_time = surv.iloc[interp_time_index].values\n",
    "            estimated_risks = 1 - surv_values_at_eval_time\n",
    "\n",
    "            if np.min(estimated_risks) == np.max(estimated_risks):\n",
    "                continue\n",
    "\n",
    "            auc = cumulative_dynamic_auc(labels_train, labels_test, estimated_risks, times=[eval_time])[0][0]\n",
    "\n",
    "            if not np.isnan(auc) and not np.isinf(auc):\n",
    "                auc_scores.append(auc)\n",
    "        except Exception as e:\n",
    "            #print(f\"AUC calculation failed: {e}, eval_time={eval_time}\")\n",
    "            pass\n",
    "\n",
    "    AUC_scores.append(np.mean(auc_scores) if auc_scores else 0.5)\n",
    "    CI.append(c_index)\n",
    "    IBS.append(brier)\n",
    "    L1_hinge.append(l1_hinge_value)\n",
    "    L1_margin.append(l1_margin_value)\n",
    "\n",
    "def safe_stat(data):\n",
    "    return round(statistics.mean(data), 3), round(statistics.stdev(data), 3) if len(data) > 1 else (0.0, 0.0)\n",
    "\n",
    "auc_mean, auc_std = safe_stat(AUC_scores)\n",
    "\n",
    "print('CI:', round(statistics.mean(CI), 3), round(statistics.stdev(CI), 3))\n",
    "print('IBS:', round(statistics.mean(IBS), 3), round(statistics.stdev(IBS), 3))\n",
    "print('L1_hinge:', round(statistics.mean(L1_hinge), 3), round(statistics.stdev(L1_hinge), 3))\n",
    "print('L1_margin:', round(statistics.mean(L1_margin), 3), round(statistics.stdev(L1_margin), 3))\n",
    "print(f'AUC: {auc_mean} ± {auc_std}')\n",
    "\n",
    "print('d_calibration_p_value:', round(d_calibration(df_test.event, surv.iloc[6])['p_value'], 3))\n",
    "print('D-Calibration:', round(sum(d_calibration(df_test.event, surv.iloc[6])['bin_proportions']), 3))\n",
    "print('d_calibration_bin_proportions:')\n",
    "for i in d_calibration(df_test.event, surv.iloc[6])['bin_proportions']:\n",
    "    print(i)\n",
    "print('D-Calibration_censored:', round(sum(d_calibration(df_test.event, surv.iloc[6])['censored_contributions']), 3))\n",
    "print('d_calibration_censored_contributions:')\n",
    "for i in d_calibration(df_test.event, surv.iloc[6])['censored_contributions']:\n",
    "    print(i)\n",
    "print('D-Calibration_uncensored:', round(sum(d_calibration(df_test.event, surv.iloc[6])['uncensored_contributions']), 3))\n",
    "print('d_calibration_uncensored_contributions:')\n",
    "for i in d_calibration(df_test.event, surv.iloc[6])['uncensored_contributions']:\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
